# Enliko Trading Platform - Scalable Architecture for 10K+ Users
# ===============================================================
# Version: 1.0.0 | Created: January 15, 2026
# ===============================================================

## üìä –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: –¢–µ–∫—É—â–µ–µ vs –¶–µ–ª–µ–≤–æ–µ

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ | –¶–µ–ª–µ–≤–æ–µ (10K+) |
|---------|---------|----------------|
| –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π | 5 | 10,000+ |
| –ü–æ–∑–∏—Ü–∏–π –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ | 50 | 100,000+ |
| API –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫ | 10 | 5,000+ |
| –°–∏–≥–Ω–∞–ª–æ–≤/–¥–µ–Ω—å | 100 | 50,000+ |
| Latency | 1-5 —Å–µ–∫ | <500ms |
| –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å | 95% | 99.9% |

---

## üèóÔ∏è –¶–ï–õ–ï–í–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           LOAD BALANCER (nginx/HAProxy)                      ‚îÇ
‚îÇ                                    ‚îÇ                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                    ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  API Server  ‚îÇ  ‚îÇ  API Server  ‚îÇ  ‚îÇ  API Server  ‚îÇ  ‚îÇ  API Server  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   (FastAPI)  ‚îÇ  ‚îÇ   (FastAPI)  ‚îÇ  ‚îÇ   (FastAPI)  ‚îÇ  ‚îÇ   (FastAPI)  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   Port 8765  ‚îÇ  ‚îÇ   Port 8766  ‚îÇ  ‚îÇ   Port 8767  ‚îÇ  ‚îÇ   Port 8768  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ         ‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                    ‚îÇ                                         ‚îÇ
‚îÇ                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ                          ‚îÇ   MESSAGE BROKER   ‚îÇ                              ‚îÇ
‚îÇ                          ‚îÇ  (Redis Streams /  ‚îÇ                              ‚îÇ
‚îÇ                          ‚îÇ    RabbitMQ)       ‚îÇ                              ‚îÇ
‚îÇ                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
‚îÇ                                    ‚îÇ                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                    ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    CELERY WORKER POOL                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Signal    ‚îÇ  ‚îÇ  Position  ‚îÇ  ‚îÇ  Position  ‚îÇ  ‚îÇ  Position  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Processor ‚îÇ  ‚îÇ  Monitor   ‚îÇ  ‚îÇ  Monitor   ‚îÇ  ‚îÇ  Monitor   ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ  Shard 1   ‚îÇ  ‚îÇ  Shard 2   ‚îÇ  ‚îÇ  Shard N   ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (signals)  ‚îÇ  ‚îÇ (users 1-  ‚îÇ  ‚îÇ(users 1K-  ‚îÇ  ‚îÇ(users NK-  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ            ‚îÇ  ‚îÇ    1000)   ‚îÇ  ‚îÇ   2000)    ‚îÇ  ‚îÇ  10000)    ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Trade    ‚îÇ  ‚îÇ    DCA     ‚îÇ  ‚îÇ   ATR      ‚îÇ                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Executor  ‚îÇ  ‚îÇ  Processor ‚îÇ  ‚îÇ  Trailing  ‚îÇ                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                    ‚îÇ                                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                    ‚ñº                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                         DATA LAYER                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   PostgreSQL     ‚îÇ  ‚îÇ      Redis       ‚îÇ  ‚îÇ   TimescaleDB    ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   (Primary DB)   ‚îÇ  ‚îÇ   (Cache/Queue)  ‚îÇ  ‚îÇ  (Time Series)   ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ users         ‚îÇ  ‚îÇ  ‚Ä¢ session cache ‚îÇ  ‚îÇ  ‚Ä¢ price_history ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ positions     ‚îÇ  ‚îÇ  ‚Ä¢ rate limiting ‚îÇ  ‚îÇ  ‚Ä¢ trade_metrics ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ trade_logs    ‚îÇ  ‚îÇ  ‚Ä¢ user_cache    ‚îÇ  ‚îÇ  ‚Ä¢ pnl_timeseries‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ signals       ‚îÇ  ‚îÇ  ‚Ä¢ price_cache   ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚Ä¢ licenses      ‚îÇ  ‚îÇ  ‚Ä¢ pub/sub       ‚îÇ  ‚îÇ                  ‚îÇ  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                         TELEGRAM BOT CLUSTER                                 ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Bot Pod 1  ‚îÇ  ‚îÇ   Bot Pod 2  ‚îÇ  ‚îÇ   Bot Pod 3  ‚îÇ  ‚îÇ   Bot Pod N  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  (Webhook)   ‚îÇ  ‚îÇ  (Webhook)   ‚îÇ  ‚îÇ  (Webhook)   ‚îÇ  ‚îÇ  (Webhook)   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîë –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´

### 1. PostgreSQL (Primary Database)

**–ü–æ—á–µ–º—É –Ω–µ SQLite:**
- SQLite: 1 writer at a time ‚Üí bottleneck –ø—Ä–∏ 10K —é–∑–µ—Ä–∞—Ö
- PostgreSQL: Connection pooling, concurrent writes, ACID

**–°—Ö–µ–º–∞ –º–∏–≥—Ä–∞—Ü–∏–∏:**
```sql
-- Users partitioned by user_id range
CREATE TABLE users (
    user_id BIGINT PRIMARY KEY,
    -- ... all existing fields
) PARTITION BY RANGE (user_id);

CREATE TABLE users_0_10k PARTITION OF users 
    FOR VALUES FROM (0) TO (10000);
CREATE TABLE users_10k_20k PARTITION OF users 
    FOR VALUES FROM (10000) TO (20000);

-- Indexes for hot queries
CREATE INDEX idx_users_active ON users(is_allowed) WHERE is_allowed = 1;
CREATE INDEX idx_positions_user_symbol ON active_positions(user_id, symbol);
CREATE INDEX idx_trade_logs_user_ts ON trade_logs(user_id, ts DESC);
```

### 2. Redis (Cache + Message Broker)

```python
# redis_client.py
import redis.asyncio as redis
from typing import Optional, Any
import json

class RedisCache:
    def __init__(self, url: str = "redis://localhost:6379"):
        self.pool = redis.ConnectionPool.from_url(url, max_connections=100)
        self.client = redis.Redis(connection_pool=self.pool)
    
    async def get_user_cache(self, user_id: int) -> Optional[dict]:
        key = f"user:{user_id}"
        data = await self.client.get(key)
        return json.loads(data) if data else None
    
    async def set_user_cache(self, user_id: int, data: dict, ttl: int = 30):
        key = f"user:{user_id}"
        await self.client.setex(key, ttl, json.dumps(data))
    
    async def rate_limit(self, user_id: int, limit: int = 60, window: int = 60) -> bool:
        """Distributed rate limiting"""
        key = f"ratelimit:{user_id}"
        current = await self.client.incr(key)
        if current == 1:
            await self.client.expire(key, window)
        return current <= limit
    
    async def publish_signal(self, signal: dict):
        """Publish signal to all workers"""
        await self.client.publish("signals", json.dumps(signal))
    
    async def get_price(self, symbol: str) -> Optional[float]:
        """Get cached price (updated by WebSocket worker)"""
        price = await self.client.hget("prices", symbol)
        return float(price) if price else None
```

### 3. Celery Task Queue

```python
# tasks/celery_app.py
from celery import Celery

app = Celery(
    'elcaro',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1',
    include=['tasks.signals', 'tasks.positions', 'tasks.trades']
)

app.conf.update(
    task_serializer='json',
    result_serializer='json',
    accept_content=['json'],
    timezone='UTC',
    enable_utc=True,
    # Worker settings
    worker_prefetch_multiplier=4,
    worker_concurrency=8,
    # Task routing
    task_routes={
        'tasks.signals.*': {'queue': 'signals'},
        'tasks.positions.*': {'queue': 'positions'},
        'tasks.trades.*': {'queue': 'trades'},
    }
)
```

```python
# tasks/positions.py
from celery import shared_task
from typing import List
import asyncio

@shared_task(bind=True, max_retries=3)
def monitor_user_positions(self, user_ids: List[int]):
    """Monitor positions for a shard of users"""
    try:
        loop = asyncio.get_event_loop()
        loop.run_until_complete(_async_monitor(user_ids))
    except Exception as exc:
        self.retry(exc=exc, countdown=5)

async def _async_monitor(user_ids: List[int]):
    """Async position monitoring for user shard"""
    from bot import monitor_positions_for_users
    await monitor_positions_for_users(user_ids)
```

### 4. User Sharding Strategy

```python
# sharding/user_shards.py
import hashlib
from typing import List, Tuple

SHARD_COUNT = 10  # 10 shards = 1000 users per shard for 10K users

def get_user_shard(user_id: int) -> int:
    """Consistent hashing for user ‚Üí shard mapping"""
    return user_id % SHARD_COUNT

def get_shard_users(shard_id: int, all_users: List[int]) -> List[int]:
    """Get all users belonging to a shard"""
    return [uid for uid in all_users if get_user_shard(uid) == shard_id]

def distribute_monitoring_tasks():
    """Distribute position monitoring across workers"""
    from db import get_active_trading_users
    from tasks.positions import monitor_user_positions
    
    users = get_active_trading_users()
    
    # Group users by shard
    shards = {}
    for uid in users:
        shard = get_user_shard(uid)
        if shard not in shards:
            shards[shard] = []
        shards[shard].append(uid)
    
    # Dispatch tasks to Celery
    for shard_id, shard_users in shards.items():
        monitor_user_positions.apply_async(
            args=[shard_users],
            queue=f'positions_shard_{shard_id}'
        )
```

### 5. WebSocket Price Feed

```python
# services/price_feed.py
import asyncio
import websockets
import json
from typing import Dict, Set
from redis_client import RedisCache

class PriceFeedService:
    """WebSocket price feed with Redis broadcasting"""
    
    def __init__(self):
        self.redis = RedisCache()
        self.subscribed_symbols: Set[str] = set()
        self.prices: Dict[str, float] = {}
    
    async def connect_bybit_ws(self):
        """Connect to Bybit WebSocket for real-time prices"""
        uri = "wss://stream.bybit.com/v5/public/linear"
        
        async with websockets.connect(uri) as ws:
            # Subscribe to tickers
            symbols = await self._get_active_symbols()
            subscribe_msg = {
                "op": "subscribe",
                "args": [f"tickers.{s}" for s in symbols]
            }
            await ws.send(json.dumps(subscribe_msg))
            
            async for message in ws:
                data = json.loads(message)
                if data.get("topic", "").startswith("tickers."):
                    await self._handle_ticker(data)
    
    async def _handle_ticker(self, data: dict):
        """Process ticker update and broadcast to Redis"""
        topic = data.get("topic", "")
        symbol = topic.replace("tickers.", "")
        ticker_data = data.get("data", {})
        
        price = float(ticker_data.get("lastPrice", 0))
        if price > 0:
            self.prices[symbol] = price
            
            # Update Redis (all workers see this instantly)
            await self.redis.client.hset("prices", symbol, str(price))
            
            # Publish price update event
            await self.redis.client.publish(
                "price_updates",
                json.dumps({"symbol": symbol, "price": price})
            )
    
    async def _get_active_symbols(self) -> Set[str]:
        """Get all symbols with open positions"""
        from db_async import get_all_active_symbols
        return await get_all_active_symbols()
```

---

## üì¶ DEPLOYMENT ARCHITECTURE

### Docker Compose (Development)

```yaml
# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: elcaro
      POSTGRES_USER: elcaro
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

  api:
    build: .
    command: uvicorn webapp.app:app --host 0.0.0.0 --port 8765 --workers 4
    environment:
      - DATABASE_URL=postgresql://elcaro:${DB_PASSWORD}@postgres:5432/elcaro
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
    ports:
      - "8765:8765"

  celery_signals:
    build: .
    command: celery -A tasks.celery_app worker -Q signals -c 4 --loglevel=info
    environment:
      - DATABASE_URL=postgresql://elcaro:${DB_PASSWORD}@postgres:5432/elcaro
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis

  celery_positions:
    build: .
    command: celery -A tasks.celery_app worker -Q positions -c 8 --loglevel=info
    environment:
      - DATABASE_URL=postgresql://elcaro:${DB_PASSWORD}@postgres:5432/elcaro
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
    deploy:
      replicas: 10  # 10 workers for 10 shards

  price_feed:
    build: .
    command: python services/price_feed.py
    environment:
      - REDIS_URL=redis://redis:6379
    depends_on:
      - redis

  telegram_bot:
    build: .
    command: python bot.py
    environment:
      - DATABASE_URL=postgresql://elcaro:${DB_PASSWORD}@postgres:5432/elcaro
      - REDIS_URL=redis://redis:6379
    depends_on:
      - postgres
      - redis
      - api

volumes:
  postgres_data:
  redis_data:
```

### Kubernetes (Production)

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elcaro-api
spec:
  replicas: 4
  selector:
    matchLabels:
      app: elcaro-api
  template:
    spec:
      containers:
      - name: api
        image: elcaro/api:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: elcaro-secrets
              key: database-url
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: elcaro-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: elcaro-api
  minReplicas: 4
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

---

## üîÑ MIGRATION PLAN

### Phase 1: Database Migration (Week 1-2)
1. ‚úÖ Set up PostgreSQL
2. ‚úÖ Create async db layer (`db_async.py`)
3. ‚úÖ Migrate schema with data
4. ‚úÖ Switch bot to PostgreSQL
5. ‚úÖ Keep SQLite as backup

### Phase 2: Redis Integration (Week 2-3)
1. ‚úÖ Deploy Redis
2. ‚úÖ Implement `RedisCache` class
3. ‚úÖ Replace in-memory caches
4. ‚úÖ Add distributed rate limiting
5. ‚úÖ Implement pub/sub for signals

### Phase 3: Celery Workers (Week 3-4)
1. ‚úÖ Set up Celery with Redis broker
2. ‚úÖ Extract signal processing to tasks
3. ‚úÖ Extract position monitoring to tasks
4. ‚úÖ Implement user sharding
5. ‚úÖ Test with load

### Phase 4: WebSocket Price Feeds (Week 4-5)
1. ‚úÖ Create PriceFeedService
2. ‚úÖ Connect to Bybit/HL WebSockets
3. ‚úÖ Broadcast prices via Redis pub/sub
4. ‚úÖ Update ATR monitoring to use cached prices
5. ‚úÖ Remove polling

### Phase 5: Horizontal Scaling (Week 5-6)
1. ‚úÖ Dockerize all components
2. ‚úÖ Set up Kubernetes cluster
3. ‚úÖ Configure auto-scaling
4. ‚úÖ Load testing (10K simulated users)
5. ‚úÖ Production deployment

---

## üìà PERFORMANCE TARGETS

| Metric | Current | Target | How |
|--------|---------|--------|-----|
| Position monitoring latency | 25s loop | <5s | Sharded workers + WebSocket |
| Signal processing | Sequential | <100ms | Celery parallel workers |
| Balance fetch | 0.4s | <100ms | Redis cache + connection pool |
| DB write throughput | 100/s | 10,000/s | PostgreSQL + connection pool |
| Concurrent users | 5 | 10,000+ | Horizontal scaling |

---

## üõ°Ô∏è RELIABILITY

### Circuit Breaker Pattern
```python
from circuitbreaker import circuit

@circuit(failure_threshold=5, recovery_timeout=30)
async def call_bybit_api(endpoint: str, params: dict):
    """API call with circuit breaker"""
    async with aiohttp.ClientSession() as session:
        async with session.get(endpoint, params=params) as resp:
            return await resp.json()
```

### Health Checks
```python
@app.get("/health")
async def health_check():
    checks = await asyncio.gather(
        check_postgres(),
        check_redis(),
        check_celery(),
        return_exceptions=True
    )
    
    status = "healthy" if all(c is True for c in checks) else "degraded"
    return {"status": status, "checks": checks}
```

### Graceful Degradation
- If Redis down ‚Üí fallback to in-memory cache
- If Celery down ‚Üí fallback to async monitoring
- If PostgreSQL down ‚Üí read from SQLite replica

---

*Document created: January 15, 2026*
*Target: 10,000+ concurrent users*
*Timeline: 6 weeks to production*
